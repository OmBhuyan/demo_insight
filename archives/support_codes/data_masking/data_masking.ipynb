{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "import string\n",
    "from more_itertools import random_permutation\n",
    "import pickle\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"/mnt/d/query_to_insight/data/masked_data/masked data backup/25apr\"\n",
    "\n",
    "\n",
    "keys_outpath = './keys_to_decode'\n",
    "outpath = '../../../data/masked_data/'\n",
    "\n",
    "os.makedirs(keys_outpath,exist_ok=True)\n",
    "os.makedirs(outpath,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(list_to_shuffle):\n",
    "    unique_entires = list_to_shuffle\n",
    "    # shifting the unique_entries by 5 places\n",
    "    encrypted_vals = unique_entires[-5:]+unique_entires[:-5]\n",
    "    out = dict(zip(unique_entires,encrypted_vals))\n",
    "    return out\n",
    "\n",
    "\n",
    "def encrypt(df, cols_to_mask, numeric_random_mask=False, seed=10, mask_dict=False):\n",
    "    try:\n",
    "        if not mask_dict:\n",
    "            mask_dict = {}\n",
    "        \n",
    "        # for numeric cols like IDs\n",
    "        if numeric_random_mask:\n",
    "            if not mask_dict:\n",
    "                digits = '0123456789'\n",
    "                if bool(seed):\n",
    "                    random.seed(seed)\n",
    "                # mask by random shuffle of digits\n",
    "                key = ''.join(random_permutation(digits))\n",
    "                for i in range(len(digits)):\n",
    "                    mask_dict[digits[i]] = key[i]\n",
    "            \n",
    "            for col in cols_to_mask:\n",
    "                df[col] = pd.to_numeric(df[col],errors='coerce').fillna(0).astype('int64')\n",
    "                df[col] = df[col].astype(str)\n",
    "                df[col] = df[col].apply(lambda word: ''.join([mask_dict[l] for l in word]))\n",
    "        \n",
    "        # for categoric cols like locations\n",
    "        else:\n",
    "            if not mask_dict:\n",
    "                for key in list(cols_to_mask.keys()):\n",
    "                    if key!='location':\n",
    "                        mask_dict[key] = shuffle(list(df[cols_to_mask[key]].melt()['value'].unique()))\n",
    "                    if key=='location':\n",
    "                        priority1_cols = cols_to_mask[key]['p1']\n",
    "                        unique_entires_p1 = list(df[priority1_cols].melt()['value'].unique())\n",
    "                        shuffled_p1_dict = shuffle(unique_entires_p1)\n",
    "                        \n",
    "                        priority2_cols = cols_to_mask[key]['p2']\n",
    "                        unique_entires_p2 = list(df[priority2_cols].melt()['value'].unique())\n",
    "                        unique_entires_p2 = list(set(unique_entires_p2)-set(unique_entires_p1))\n",
    "                        shuffled_p2_dict = shuffle(unique_entires_p2)\n",
    "                        \n",
    "                        # appending 2 dicts\n",
    "                        mask_dict[key] = {**shuffled_p2_dict,**shuffled_p1_dict}\n",
    "            \n",
    "            for key in list(cols_to_mask.keys()):\n",
    "                if key!='location':\n",
    "                    for col in cols_to_mask[key]:\n",
    "                        df[col] = df[col].map(mask_dict[key])\n",
    "                else:\n",
    "                    cols = sum([v for k,v in cols_to_mask[key].items()], [])\n",
    "                    for col in cols:\n",
    "                        df[col] = df[col].map(mask_dict[key])\n",
    "    except Exception as e:\n",
    "        print('keyerror: ',e)\n",
    "        print(traceback.format_exc())\n",
    "    return df, mask_dict\n",
    "\n",
    "\n",
    "\n",
    "def decrypt(df, cols_to_mask, mask_dict, numeric_random_mask=False):\n",
    "    decode = {}\n",
    "    \n",
    "    # for numeric cols like IDs\n",
    "    if numeric_random_mask:\n",
    "        decode = {v:k for k,v in mask_dict.items()}\n",
    "        for col in cols_to_mask:\n",
    "            df[col] = pd.to_numeric(df[col],errors='coerce').fillna(0).astype('int64')\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].apply(lambda word: ''.join([decode[l] for l in word]))\n",
    "    \n",
    "    # for categoric cols like locations\n",
    "    else:\n",
    "        for key in list(cols_to_mask.keys()):\n",
    "            decode[key] = {v:k for k,v in mask_dict[key].items()}\n",
    "            for col in cols_to_mask[key]:\n",
    "                df[col] = df[col].map(decode[key])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 3, 'c': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {'a': 1, 'b': 2}\n",
    "y = {'b': 3, 'c': 4}\n",
    "\n",
    "{**x,**y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data = pd.read_csv(data_path)\n",
    "\n",
    "cols_to_mask = {'location':{'p1':['source_location_name',  'pick_locationname'], \n",
    "                            'p2':['destination_location_name', 'drop_locationname']}, \n",
    "                'carrier':['carrier_name']}\n",
    "\n",
    "num_cols_to_mask = ['dps_tm_load_id','dps_tripid']\n",
    "\n",
    "\n",
    "temp = stream_data.copy()\n",
    "masked_stream_data,mask_dict = encrypt(temp.copy(), cols_to_mask)\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(mask_dict, f)\n",
    "\n",
    "masked_stream_data_,num_mask_dict = encrypt(masked_stream_data, num_cols_to_mask, numeric_random_mask=True)\n",
    "\n",
    "with open(os.path.join(keys_outpath,'num_cols.pkl'), 'wb') as f:\n",
    "    pickle.dump(mask_dict, f)\n",
    "\n",
    "for col in ['destination_location_name', 'source_location_name','pick_locationname','drop_locationname','carrier_name','sap_material_description']:\n",
    "    if col in masked_stream_data_.columns:\n",
    "        masked_stream_data_[col] = masked_stream_data_[col].astype(str).apply(lambda x: x.replace('FRITO LAY', 'CHIPS_'))\n",
    "        masked_stream_data_[col] = masked_stream_data_[col].astype(str).apply(lambda x: x.replace('FRITOLAY', 'CHIPS'))\n",
    "        masked_stream_data_[col] = masked_stream_data_[col].astype(str).apply(lambda x: x.replace('FRITO', 'CHIP'))\n",
    "        \n",
    "    \n",
    "\n",
    "masked_stream_data_.to_excel(os.path.join(outpath,'masked_stream_data.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_location_name</th>\n",
       "      <th>source_location_name</th>\n",
       "      <th>carrier_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Location 96</td>\n",
       "      <td>Location 0</td>\n",
       "      <td>Carrier 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location 29</td>\n",
       "      <td>Location 1</td>\n",
       "      <td>Carrier 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location 97</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Carrier 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location 98</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Carrier 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location 99</td>\n",
       "      <td>Location 0</td>\n",
       "      <td>Carrier 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  destination_location_name source_location_name carrier_name\n",
       "0               Location 96           Location 0    Carrier 0\n",
       "1               Location 29           Location 1    Carrier 1\n",
       "2               Location 97           Location 2    Carrier 2\n",
       "3               Location 98           Location 2    Carrier 2\n",
       "4               Location 99           Location 0    Carrier 0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_data_file = \"masked_invoice_data_11apr.csv\"\n",
    "# prod_data_file = \"masked_product_data_11apr.csv\"\n",
    "# desc_data_file = \"material_descriptions_masked.csv\"\n",
    "\n",
    "invoice_data = pd.read_csv(os.path.join(data_path, invoice_data_file))\n",
    "# prod_data = pd.read_csv(os.path.join(data_path, prod_data_file))\n",
    "# desc_data = pd.read_csv(os.path.join(data_path, desc_data_file))\n",
    "\n",
    "\n",
    "# location masking\n",
    "unique_vals = invoice_data[['source_location_name', 'destination_location_name']].melt()['value'].unique()\n",
    "masked_val = [f\"Location {x}\" for x in range(len(unique_vals))]\n",
    "location_masking_dict = dict(zip(unique_vals, masked_val))\n",
    "\n",
    "# carrier_name masking\n",
    "unique_vals = invoice_data['carrier_name'].unique()\n",
    "masked_val = [f\"Carrier {x}\" for x in range(len(unique_vals))]\n",
    "carrier_masking_dict = dict(zip(unique_vals, masked_val))\n",
    "\n",
    "# masking\n",
    "invoice_data['source_location_name'] = invoice_data['source_location_name'].map(location_masking_dict)\n",
    "invoice_data['destination_location_name'] = invoice_data['destination_location_name'].map(location_masking_dict)\n",
    "invoice_data['carrier_name'] = invoice_data['carrier_name'].map(carrier_masking_dict)\n",
    "\n",
    "invoice_data.to_csv(os.path.join(outpath, invoice_data_file), index=False)\n",
    "\n",
    "pd.DataFrame(location_masking_dict, index=['masked_value']).T.reset_index().to_csv(os.path.join(keys_outpath, \"location_masking_keys.csv\"), index=False)\n",
    "pd.DataFrame(carrier_masking_dict, index=['masked_value']).T.reset_index().to_csv(os.path.join(keys_outpath, \"carrier_masking_keys.csv\"), index=False)\n",
    "\n",
    "\n",
    "invoice_data[['destination_location_name', 'source_location_name','carrier_name']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update mask dict with model ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_entried_to_maskdict(old_df, new_df, mask_dict, cols_to_mask):\n",
    "    for key in list(cols_to_mask.keys()):\n",
    "        if key=='location':\n",
    "            priority1_cols = cols_to_mask[key]['p1']\n",
    "            old_unique_entires_p1 = list(old_df[priority1_cols].melt()['value'].dropna().unique())\n",
    "            new_unique_entires_p1 = list(new_df[priority1_cols].melt()['value'].dropna().unique())\n",
    "            newly_added_p1 = list(set(new_unique_entires_p1)-set(old_unique_entires_p1))\n",
    "            shuffled_newly_added_p1 = shuffle(newly_added_p1)\n",
    "            \n",
    "            priority2_cols = cols_to_mask[key]['p2']\n",
    "            old_unique_entires_p2 = list(old_df[priority2_cols].melt()['value'].dropna().unique())\n",
    "            new_unique_entires_p2 = list(new_df[priority2_cols].melt()['value'].dropna().unique())\n",
    "            newly_added_p2 = list(set(new_unique_entires_p2)-set(old_unique_entires_p2))\n",
    "            newly_added_p2 = list(set(newly_added_p2)-set(newly_added_p1))\n",
    "            shuffled_newly_added_p2 = shuffle(newly_added_p2)\n",
    "            \n",
    "            # appending 2 dicts\n",
    "            appended = {**shuffled_newly_added_p2,**shuffled_newly_added_p1}\n",
    "            mask_dict[key] = {**appended,**mask_dict[key]}\n",
    "        else:\n",
    "            new_unique_entires = list(new_df[cols_to_mask[key]].melt()['value'].dropna().unique())\n",
    "            old_unique_entires = list(mask_dict[key].keys())\n",
    "            newly_added = list(set(new_unique_entires)-set(old_unique_entires))\n",
    "            shuffled_newly_added = shuffle(newly_added)\n",
    "            mask_dict[key] = {**shuffled_newly_added,**mask_dict[key]}\n",
    "\n",
    "    \n",
    "    return mask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250/3494795631.py:2: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  total_trip_data = pd.read_excel(transport, sheet_name=\"Total_Trip_Data\")\n"
     ]
    }
   ],
   "source": [
    "model_data = pd.read_csv(model_ready_data)\n",
    "total_trip_data = pd.read_excel(transport, sheet_name=\"Total_Trip_Data\")\n",
    "\n",
    "num_cols_to_mask = ['dps_tm_load_id','dps_tripid']\n",
    "cols_to_mask = {'location':{'p1':['source_location_name',  'pick_locationname'], \n",
    "                            'p2':['destination_location_name', 'drop_locationname']}, \n",
    "                'carrier':['carrier_name']}\n",
    "\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "with open(os.path.join(keys_outpath,'num_cols.pkl'), 'rb') as f:\n",
    "    num_mask_dict = pickle.load(f)\n",
    "\n",
    "new_mask_dict = add_new_entried_to_maskdict(total_trip_data, model_data, mask_dict=mask_dict.copy(), cols_to_mask=cols_to_mask)\n",
    "\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols_with_model_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(mask_dict, f)\n",
    "    \n",
    "for key in list(cols_to_mask.keys()):\n",
    "    if key!='location':\n",
    "        for col in cols_to_mask[key]:\n",
    "            model_data[col] = model_data[col].map(new_mask_dict[key])\n",
    "    else:\n",
    "        cols = sum([v for k,v in cols_to_mask[key].items()], [])\n",
    "        for col in cols:\n",
    "            model_data[col] = model_data[col].map(new_mask_dict[key])\n",
    "\n",
    "model_data,num_mask_dict = encrypt(model_data, num_cols_to_mask, mask_dict=num_mask_dict, numeric_random_mask=True)\n",
    "    \n",
    "model_data.to_csv(os.path.join(outpath,'masked_model_data.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 647)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_mask_dict['location'].keys()),len(mask_dict['location'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FRITO LAY DC BATON ROUGE DC550</th>\n",
       "      <td>SCOTT DEPOT DC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRITO LAY DC SCOTT DEPOT</th>\n",
       "      <td>MENASHA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOUNTAIN HOME ID RELAY</th>\n",
       "      <td>FRITO LAY DC BATON ROUGE DC550</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCOTT DEPOT DC</th>\n",
       "      <td>FRANKFORT PLANT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRITO LAY DC TULSA</th>\n",
       "      <td>FRITO LAY DC SCOTT DEPOT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MENASHA</th>\n",
       "      <td>FRITO LAY PL FRANKFORT CORE IN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TULSA DC</th>\n",
       "      <td>WESTROCK CP LLC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOUTH BATON ROUGE DC550</th>\n",
       "      <td>MOUNTAIN HOME ID RELAY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRITO LAY CP TECH ECOMM</th>\n",
       "      <td>BELTON DC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLEN SW CUSTOM</th>\n",
       "      <td>FRITO LAY RP DGN MARKETING</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHEARERS FT WORTH DIRECT</th>\n",
       "      <td>FRITO LAY RP ALLEN SW CUSTOM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FRITO LAY RP ALLEN SW CUSTOM</th>\n",
       "      <td>SHEARERS FT WORTH DIRECT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATCO</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0    1\n",
       "FRITO LAY DC BATON ROUGE DC550                  SCOTT DEPOT DC  NaN\n",
       "FRITO LAY DC SCOTT DEPOT                               MENASHA  NaN\n",
       "MOUNTAIN HOME ID RELAY          FRITO LAY DC BATON ROUGE DC550  NaN\n",
       "SCOTT DEPOT DC                                 FRANKFORT PLANT  NaN\n",
       "FRITO LAY DC TULSA                    FRITO LAY DC SCOTT DEPOT  NaN\n",
       "MENASHA                         FRITO LAY PL FRANKFORT CORE IN  NaN\n",
       "TULSA DC                                       WESTROCK CP LLC  NaN\n",
       "SOUTH BATON ROUGE DC550                 MOUNTAIN HOME ID RELAY  NaN\n",
       "FRITO LAY CP TECH ECOMM                              BELTON DC  NaN\n",
       "ALLEN SW CUSTOM                     FRITO LAY RP DGN MARKETING  NaN\n",
       "SHEARERS FT WORTH DIRECT          FRITO LAY RP ALLEN SW CUSTOM  NaN\n",
       "FRITO LAY RP ALLEN SW CUSTOM          SHEARERS FT WORTH DIRECT  NaN\n",
       "ATCO                                                       NaN  NaN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = pd.DataFrame(new_mask_dict['location'], index=[0]).T.merge(pd.DataFrame(mask_dict['location'], index=[1]).T,left_index=True, right_index=True, how='left')\n",
    "aa[aa[0]!=aa[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority1_cols = cols_to_mask['location']['p1']\n",
    "unique_entires_p1 = list(masked_total_trip_data_[priority1_cols].melt()['value'].unique())\n",
    "\n",
    "priority2_cols = cols_to_mask['location']['p2']\n",
    "unique_entires_p2 = list(masked_total_trip_data_[priority2_cols].melt()['value'].unique())\n",
    "unique_entires_p2 = list(set(unique_entires_p2)-set(unique_entires_p1))\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "    \n",
    "c = 0\n",
    "for k,v in mask_dict['location'].items():\n",
    "    if v in (unique_entires_p1):\n",
    "        if k in (unique_entires_p2):\n",
    "            c=c+1\n",
    "    if v in (unique_entires_p2):\n",
    "        if k in (unique_entires_p1):\n",
    "            c=c+1\n",
    "    \n",
    "c     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_completed_trips = pd.read_excel(sample_trips)\n",
    "\n",
    "cols_to_mask = {'location':{'p1':['source_location_name',  'pick_locationname'], \n",
    "                            'p2':['destination_location_name', 'drop_locationname']}, \n",
    "                'carrier':['carrier_name']}\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "\n",
    "masked_sample_completed_trips,temp = encrypt(sample_completed_trips, cols_to_mask, mask_dict=mask_dict)\n",
    "masked_sample_completed_trips_,mask_dict = encrypt(masked_sample_completed_trips, num_cols_to_mask, numeric_random_mask=True)\n",
    "masked_sample_completed_trips_.to_excel(os.path.join(outpath,'masked_sample_completed_trips.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_data = pd.read_excel(transport, sheet_name=\"Material_Data\")\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "\n",
    "cols_to_mask = {'carrier':['carrier_name']}\n",
    "num_cols_to_mask = ['sap_tm_load_id']\n",
    "\n",
    "masked_material_data,temp = encrypt(material_data, cols_to_mask, mask_dict=mask_dict)\n",
    "masked_material_data_,mask_dict = encrypt(masked_material_data, num_cols_to_mask, numeric_random_mask=True)\n",
    "masked_material_data_.to_excel(os.path.join(outpath,'masked_material_data.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrypting the masked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec = pd.read_excel(\"masked_data\\masked_total_trip_data.xlsx\")\n",
    "\n",
    "# cols_to_mask = {'carrier':['carrier_name']}\n",
    "# num_cols_to_mask = ['sap_tm_load_id']\n",
    "cols_to_mask = {'location':['source_location_name', 'destination_location_name', 'pick_locationname', 'drop_locationname'], \n",
    "                'carrier':['carrier_name']}\n",
    "num_cols_to_mask = ['dps_tm_load_id','dps_tripid']\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "test_dec_ = decrypt(test_dec, cols_to_mask, mask_dict, numeric_random_mask=False)\n",
    "\n",
    "with open(os.path.join(keys_outpath,'num_cols.pkl'), 'rb') as f:\n",
    "    num_mask_dict = pickle.load(f)\n",
    "test_dec__ = decrypt(test_dec_, num_cols_to_mask, num_mask_dict, numeric_random_mask=True)\n",
    "\n",
    "test_dec__.to_excel(os.path.join(outpath,'test_masked_total_trip_data.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec = pd.read_excel(\"masked_data\\masked_sample_completed_trips.xlsx\")\n",
    "\n",
    "# cols_to_mask = {'carrier':['carrier_name']}\n",
    "# num_cols_to_mask = ['sap_tm_load_id']\n",
    "cols_to_mask = {'location':['source_location_name', 'destination_location_name', 'pick_locationname', 'drop_locationname'], \n",
    "                'carrier':['carrier_name']}\n",
    "num_cols_to_mask = ['dps_tm_load_id','dps_tripid']\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "test_dec_ = decrypt(test_dec, cols_to_mask, mask_dict, numeric_random_mask=False)\n",
    "\n",
    "with open(os.path.join(keys_outpath,'num_cols.pkl'), 'rb') as f:\n",
    "    num_mask_dict = pickle.load(f)\n",
    "test_dec__ = decrypt(test_dec_, num_cols_to_mask, num_mask_dict, numeric_random_mask=True)\n",
    "\n",
    "test_dec__.to_excel(os.path.join(outpath,'test_masked_sample_completed_trips.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec = pd.read_excel(\"masked_data\\masked_material_data.xlsx\")\n",
    "\n",
    "cols_to_mask = {'carrier':['carrier_name']}\n",
    "num_cols_to_mask = ['sap_tm_load_id']\n",
    "\n",
    "with open(os.path.join(keys_outpath,'cat_cols.pkl'), 'rb') as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "test_dec_ = decrypt(test_dec, cols_to_mask, mask_dict, numeric_random_mask=False)\n",
    "\n",
    "with open(os.path.join(keys_outpath,'num_cols.pkl'), 'rb') as f:\n",
    "    num_mask_dict = pickle.load(f)\n",
    "test_dec__ = decrypt(test_dec_, num_cols_to_mask, num_mask_dict, numeric_random_mask=True)\n",
    "\n",
    "test_dec__.to_excel(os.path.join(outpath,'test_masked_material_data.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
