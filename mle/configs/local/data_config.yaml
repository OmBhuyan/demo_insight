path:
  # Root folder of the project
  parent: .
  # Path where the input data, db files, output folder (will be created in the code) are saved
  input_data_path: ${path.parent}/data/db
  # add an enty for each table. in case of multiple tables, add multiple entries
  # Example, for `stream_data: "masked_stream_data_21mar.csv"`, SQL table name will be stream_data. Also "stream_data.json" must be present in data_dictionary_path
  # in case of multiple tables, code read all jsons from the data_dictionary_pathm append them and feed it to the model
  input_file_names:
    invoice_data: invoice_data_masked.csv
    material_desc: material_descriptions_masked.csv
    product_data: product_data_masked.csv

  # Path where the data dictionaries are available (one json file per one table mentioned in path.input_file_name section of this config)
  data_dictionary_path: ${path.parent}/data/data_dictionary

  # provide path to the txt file containing business overview. Use it only with GPT4
  business_overview_path:

  # Path where the API key is stored. This will be added as a temporary environment variable in the code
  api_key_location: ${path.parent}/data/tmp/azure_api_key_gpt4
  # Output path where results will be saved
  output_path: ${path.parent}/data/output_folder
  # Experiment name. Sub folder inside output_path will be created with this name. All results of runs will be saved inside `output_path/exp_name`
  exp_name: streamlit_knowledge_base

# Database connection parameters
db_params:
  # Database name. mysql and sqlite are supported at the moment
  db_name: "sqlite"
  # Hostname
  host: "localhost"
  # Username
  username: "root"
  # Path where the password to db connection is stored (valid when db_params.db_name is mysql)
  password_path: #"/mnt/c/Users/chetan.subramani/Downloads/insights_generator/password_db/password.txt"
  # Path where the db file for database is stored. Can be created using a notebook in support codes of this project
  sqlite_database_path: ${path.parent}/data/db/database.db
  chunk_size: 500000 #500000

cloud_storage:
  #TODO: This should go in the same place where Azure OpenAI key goes
  # Add connection parameters for DBFS. Leave it blank for normal filestorage (Linux, Windows)
  # Add the path in local FS and not in the cloud since we don't have access to the DBFS where this is read - this is used for establishing the connection
  connection_params_path:
